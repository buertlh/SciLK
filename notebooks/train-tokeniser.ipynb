{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train a tokeniser using our break-and-stitch strategy. API-wise a tokeniser is a function of form `(Text) -> List[Interval[Text]]` (where `Interval` is defined in `scilk.util.intervals`), that is for any text X it returns an ordered sequence of nonoverlapping ranges corresponding to individual tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence, Iterable, Iterator, Tuple, List, Mapping, Any, Optional, Callable, TypeVar\n",
    "from itertools import dropwhile, groupby, chain, starmap\n",
    "from functools import reduce\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "import operator as op\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import joblib\n",
    "from fn import F\n",
    "from fn.iters import splitby, droplast\n",
    "from binpacking import to_constant_bin_number\n",
    "from keras import layers, models\n",
    "\n",
    "from scilk.corpora import chemdner, corpus\n",
    "from scilk.collections import common\n",
    "from scilk.util import preprocessing, binning, segments\n",
    "from scilk.util.intervals import Interval\n",
    "from scilk.util.patterns import ptokenise, ptransform\n",
    "from scilk.util.networks import wrappers, blocks, callbacks\n",
    "\n",
    "T = TypeVar('T')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse and encode the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemnder_train = corpus.flatten_abstracts(\n",
    "    chemdner.parse('data/chemdner_corpus/training.abstracts.txt',\n",
    "                   'data/chemdner_corpus/training.annotations.txt')\n",
    ")\n",
    "chemnder_dev = corpus.flatten_abstracts(\n",
    "    chemdner.parse('data/chemdner_corpus/development.abstracts.txt',\n",
    "                   'data/chemdner_corpus/development.annotations.txt')\n",
    ")\n",
    "\n",
    "texts, annotations, borders = zip(*chemnder_train+chemnder_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maxcharval, charmap, text_encoder = common.build_charencoder(''.join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an extra-fine primary tokenisation (i.e. the \"break\" step)\n",
    "tokeniser = F(ptokenise, [re.compile('\\w+|[^\\s\\w]')])\n",
    "tokens = (F(map, tokeniser) >> list)(texts)\n",
    "\n",
    "# find the stitchpoints\n",
    "token_stitches = list(starmap(segments.stitchpoints, zip(tokens, annotations)))\n",
    "\n",
    "# since we are not going to stitch words on basic white-space characters, we filter these points out\n",
    "def filter_stitches(text, points) -> List[int]:\n",
    "    return [point for point in points if ' ' not in text[point:point+2]]\n",
    "\n",
    "filtered_stitches = [filter_stitches(text, points) for text, points in zip(texts, token_stitches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset for stateful-learning using reversible binpacking \n",
    "chunksize = 256\n",
    "batchsize = 64\n",
    "\n",
    "encoded_texts, encoded_stitches = (\n",
    "    F(map, lambda text, points: (text_encoder(text), encode_annotation(len(text), points))) >> \n",
    "    (map, F(map, reverse)) >> \n",
    "    (lambda x: zip(*x)) >>\n",
    "    (map, np.array)\n",
    ")(texts, filtered_stitches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate training and validation data\n",
    "\n",
    "indices = np.arange(len(texts))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "trainsplit = 0.9\n",
    "train_split = indices[:int(len(indices)*trainsplit)]\n",
    "val_split = indices[int(len(indices)*trainsplit):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the datasets\n",
    "\n",
    "texts_train, anno_train = [arr[train_split] for arr in [encoded_texts, encoded_stitches]]\n",
    "texts_val, anno_val = [arr[val_split] for arr in [encoded_texts, encoded_stitches]]\n",
    "\n",
    "bins_train, bins_val = map(F(binning.binpack, batchsize, len), [texts_train, texts_val])\n",
    "\n",
    "x_train, y_train = map(F(binning.merge_bins, bins=bins_train), [texts_train, anno_train])\n",
    "x_val, y_val = map(F(binning.merge_bins, bins=bins_val), [texts_val, anno_val])\n",
    "\n",
    "x_batches_train, y_batches_train = map(F(preprocessing.chunksteps, chunksize), [x_train, y_train])\n",
    "x_batches_val, y_batches_val = map(F(preprocessing.chunksteps, chunksize), [x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batches_train.shape, y_batches_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build and train the network, save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(batch_shape=(batchsize, chunksize))\n",
    "embeddings = layers.Embedding(maxcharval+1, 32)(inputs)\n",
    "l_cnn = blocks.cnn([256, 256], 3, [0.3, None])\n",
    "l_rnn1 = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(16, stateful=True, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "l_rnn2 = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(16, stateful=True, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "l_rnn3 = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(16, stateful=True, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "\n",
    "# Keras' Dense layers now behave like TimeDistributed layers when applied to sequential inputs\n",
    "labels = layers.Dense(1, activation='sigmoid')(\n",
    "    reduce(lambda graph, layer: layer(graph), [l_cnn, l_rnn1, l_rnn2, l_rnn3], embeddings)\n",
    ")\n",
    "\n",
    "model = models.Model(inputs, labels)\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our networks have stateful layers, we need to reset them between epochs.\n",
    "callables = [(lambda _: l_rnn1.reset_states()), \n",
    "             (lambda _: l_rnn2.reset_states()), \n",
    "             (lambda _: l_rnn3.reset_states())]\n",
    "resetter = callbacks.Caller({'on_epoch_begin': callables, 'on_epoch_end': callables})\n",
    "\n",
    "# prepare the validators; the validators will keep track of the F1 scores on the validation dataset \n",
    "# and save model weights upon performance improvements\n",
    "scores = {'precision': F(metrics.precision_score, average='binary', labels=[1]),\n",
    "          'recall': F(metrics.recall_score, average='binary', labels=[1]),\n",
    "          'f1': F(metrics.f1_score, average='binary', labels=[1])}\n",
    "\n",
    "! mkdir -p trainlogs\n",
    "logfile = open('trainlogs/stitches-text.log', 'w')\n",
    "validator = callbacks.Validator([np.vstack(x_batches_val)], np.vstack(y_batches_val).flatten(), \n",
    "                                batchsize, scores, lambda pred: (np.vstack(pred) > 0.5).astype(int).flatten(), \n",
    "                                'precision', prefix=f'trainlogs/stitches-text', stream=logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(np.vstack(x_batches_train), np.vstack(y_batches_train)[:,:,None],\n",
    "          batch_size=batchsize, shuffle=False, epochs=45, callbacks=[resetter, validator])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
