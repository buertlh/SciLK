{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will train an NER model identical to the one described in our publication. This step assumes that you already have a trained tokeniser, though if you don't want to use our trainable tokenisers, it's quite trivial to use another one, because API-wise a tokeniser is simply a function of form `(Text) -> List[Interval[Text]]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Union, Iterable, Sequence\n",
    "from functools import reduce\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'  # you might want to change this\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from fn import F\n",
    "from sklearn import metrics\n",
    "from keras import layers, models\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "\n",
    "from scilk.corpora import corpus, chemdner\n",
    "from scilk.util import preprocessing, intervals, binning\n",
    "from scilk.collections import common\n",
    "from scilk.collections.chemdner import loaders\n",
    "from scilk.util.networks import blocks, wrappers, callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Before we start, let's load the prerequisites: a trained tokeniser and token encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root = 'chemdner-collection'  # our recurring destination root\n",
    "\n",
    "tokeniser_data = {\n",
    "    'tokeniser_weights': f'{root}/tokeniser-weights.hdf5',\n",
    "    'charmap': f'{root}/charmap.joblib'\n",
    "}\n",
    "tokeniser = loaders.load_tokeniser(tokeniser_data)\n",
    "\n",
    "# word embeddings pretrained in Glove\n",
    "word_transform = lambda word: '<NUM>' if word.isnumeric() else word\n",
    "glove_embeddings = common.read_glove(f'{root}/vectors.txt.gz')\n",
    "wordencoder = common.build_wordencoder(glove_embeddings, word_transform)\n",
    "\n",
    "\n",
    "# we reuse the character set from tokeniser_data['charmap'] here\n",
    "wordlen = 32  # specify the maximum number of characters to keep in a word\n",
    "maxchar, charmap, charencoder = common.build_charencoder(joblib.load(tokeniser_data['charmap']), wordlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here are the major steps:\n",
    "\n",
    "1. Read the corpus (texts and annotations) and tokenise texts;\n",
    "2. Encode tokens and annotations;\n",
    "3. Separate datasets for training and validation;\n",
    "4. Binpack and chunk the datasets;\n",
    "5. Build and train the network, save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the corpus (texts and annotations) and tokenise texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemnder_train = corpus.flatten_abstracts(\n",
    "    chemdner.parse('data/chemdner_corpus/training.abstracts.txt',\n",
    "                   'data/chemdner_corpus/training.annotations.txt')\n",
    ")\n",
    "chemnder_dev = corpus.flatten_abstracts(\n",
    "    chemdner.parse('data/chemdner_corpus/development.abstracts.txt',\n",
    "                   'data/chemdner_corpus/development.annotations.txt')\n",
    ")\n",
    "texts, annotations, _ = zip(*chemnder_train+chemnder_dev)\n",
    "tokenised = tokeniser(list(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Encode tokens and annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our model has two separate output nodes with labels for the entity starts and parts, \n",
    "# we need to encode these separately\n",
    "\n",
    "\n",
    "def encode_parts(timesteps: Sequence[intervals.Interval], \n",
    "                 annotations: Sequence[intervals.Interval[str]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    !NOTE! it returns a sum, letting one trace overlapping entities\n",
    "    \"\"\"\n",
    "    encoded_points = np.zeros(max(step.stop for step in timesteps), dtype=np.int32)\n",
    "    for iv in annotations:\n",
    "        encoded_points[iv.start:iv.stop] = 1\n",
    "    return np.array([encoded_points[step.start:step.stop].sum() for step in timesteps], dtype=np.int32)\n",
    "\n",
    "\n",
    "def encode_starts(timesteps: Sequence[intervals.Interval], \n",
    "                  annotations: Sequence[intervals.Interval[str]]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    !NOTE! it returns a sum, letting one trace overlapping entities\n",
    "    \"\"\"\n",
    "    encoded_points = np.zeros(max(step.stop for step in timesteps), dtype=np.int32)\n",
    "    encoded_points[[iv.start for iv in annotations]] = 1\n",
    "    return np.array([encoded_points[step.start:step.stop].sum() for step in timesteps], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pair(tokens, annotations):\n",
    "    strings = list(intervals.unload(tokens))\n",
    "    charemb = charencoder(strings)\n",
    "    wordemb = wordencoder(strings)\n",
    "    parts = encode_parts(tokens, annotations)\n",
    "    starts = encode_starts(tokens, annotations)\n",
    "    return charemb, wordemb, parts, starts\n",
    "\n",
    "\n",
    "charembs, wordembs, parts, starts = (\n",
    "    F(map, process_pair) >> \n",
    "    (lambda x: zip(*x)) >>\n",
    "    (map, np.array)\n",
    ")(tokenised, annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Separate datasets for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(texts))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "trainsplit = 0.8\n",
    "train_split = indices[:int(len(indices)*trainsplit)]\n",
    "val_split = indices[int(len(indices)*trainsplit):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Binpack and chunk the datasets;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chunksize = 128\n",
    "batchsize = 32\n",
    "\n",
    "charembs_train, wordembs_train, parts_train, starts_train = [\n",
    "    arr[train_split] for arr in [charembs, wordembs, parts, starts]\n",
    "]\n",
    "charembs_val, wordembs_val, parts_val, starts_val = [\n",
    "    arr[val_split] for arr in [charembs, wordembs, parts, starts]\n",
    "]\n",
    "\n",
    "bins_train, bins_val = map(F(binning.binpack, batchsize, len), [parts_train, parts_val])\n",
    "\n",
    "x_char_train, x_word_train, y_parts_train, y_starts_train = map(\n",
    "    F(binning.merge_bins, bins=bins_train) >> F(preprocessing.chunksteps, chunksize), \n",
    "    [charembs_train, wordembs_train, parts_train, starts_train]\n",
    ")\n",
    "x_char_val, x_word_val, y_parts_val, y_starts_val = map(\n",
    "    F(binning.merge_bins, bins=bins_val) >> F(preprocessing.chunksteps, chunksize), \n",
    "    [charembs_val, wordembs_val, parts_val, starts_val]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_char_train.shape, x_word_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Build and train the network, save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chunksize = 128\n",
    "batchsize = 32\n",
    "wordemb_dim = glove_embeddings.shape[1]\n",
    "charemb_dim = 32\n",
    "charemb_units = 16\n",
    "\n",
    "# character block\n",
    "inputs_char = layers.Input(batch_shape=(batchsize, chunksize, wordlen))\n",
    "char_embeddings = blocks.charemb(maxchar+1, chunksize, charemb_dim,\n",
    "                                 charemb_units, 0.3, 0.3, mask=False,\n",
    "                                 layer=layers.GRU)(inputs_char)\n",
    "char_conv = blocks.cnn([256, 256], 3, [0.3, None], \n",
    "                       name_template='narrowcharconv{}')(char_embeddings)\n",
    "\n",
    "# word block\n",
    "inputs_word = layers.Input(batch_shape=(batchsize, chunksize, wordemb_dim))\n",
    "word_conv = blocks.cnn([256, 256], 3, [0.3, None], \n",
    "                       name_template='narrowwordconv{}')(inputs_word)\n",
    "\n",
    "# join CNN-extracted features and reshape\n",
    "def reshape(shape, layer):\n",
    "    return layers.Lambda(\n",
    "        lambda incomming: K.reshape(incomming, shape=shape)\n",
    "    )(layer)\n",
    "\n",
    "feat_shape = [batchsize, chunksize, -1]\n",
    "feat_layers = [char_conv, word_conv]\n",
    "features = reshape(feat_shape, layers.concatenate(feat_layers, axis=-1))\n",
    "\n",
    "# RNN-blocks shared by two output nodes\n",
    "rnn_shared1 = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(32, stateful=True, dropout=0.3, recurrent_dropout=0.3,\n",
    "               return_sequences=True))\n",
    "rnn_shared2 = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(32, stateful=True, dropout=0.3, recurrent_dropout=0.3,\n",
    "               return_sequences=True))\n",
    "shared_rnn_layers = [rnn_shared1, rnn_shared2]\n",
    "rnn_shared = reduce(lambda graph, layer: layer(graph), shared_rnn_layers,\n",
    "                    features)\n",
    "\n",
    "# separate branch for parts-detection \n",
    "rnn_parts = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(32, stateful=True, dropout=0.3, recurrent_dropout=0.3,\n",
    "               return_sequences=True))\n",
    "labels_parts = layers.Dense(1, activation='sigmoid')(rnn_parts(rnn_shared))\n",
    "attention = reshape(feat_shape, layers.multiply([labels_parts, rnn_shared]))\n",
    "# separate branch for starts-detection \n",
    "rnn_starts = wrappers.HalfStatefulBidirectional(\n",
    "    layers.GRU(32, stateful=True, dropout=0.3, recurrent_dropout=0.3,\n",
    "               return_sequences=True))\n",
    "labels_starts = layers.Dense(1, activation='sigmoid')(rnn_starts(attention))\n",
    "\n",
    "# compile the model\n",
    "model = models.Model([inputs_char, inputs_word], [labels_parts, labels_starts])\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "stateful_layers = [*shared_rnn_layers, rnn_parts, rnn_starts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input and output data for training and validation\n",
    "inputs = [np.vstack(x_char_train), np.vstack(x_word_train)]\n",
    "output_parts = np.clip(np.vstack(y_parts_train), 0, 1)[:,:,None]\n",
    "output_starts = np.clip(np.vstack(y_starts_train), 0, 1)[:,:,None]\n",
    "inputs_val = [np.vstack(x_char_val), np.vstack(x_word_val)]\n",
    "output_parts_val = np.clip(np.vstack(y_parts_val), 0, 1).flatten()\n",
    "output_starts_val = np.clip(np.vstack(y_starts_val), 0, 1).flatten()\n",
    "\n",
    "# since our networks have stateful layers, we need to reset them between epochs.\n",
    "def reset_stateful_layers(layers, _):\n",
    "    for layer in layers:\n",
    "        layer.reset_states()\n",
    "\n",
    "resetter = callbacks.Caller({'on_epoch_begin': [F(reset_stateful_layers, stateful_layers)], \n",
    "                             'on_epoch_end': [F(reset_stateful_layers, stateful_layers)]})\n",
    "\n",
    "# prepare the validators; the validators will keep track of the F1 scores on the validation dataset \n",
    "# and save model weights upon performance improvements\n",
    "! mkdir -p trainlogs\n",
    "\n",
    "scores = {\"precision\": F(metrics.precision_score, average=\"binary\", labels=[1]),\n",
    "          \"recall\": F(metrics.recall_score, average=\"binary\", labels=[1]),\n",
    "          \"f1\": F(metrics.f1_score, average=\"binary\", labels=[1])}\n",
    "\n",
    "logfile = open(f\"trainlogs/ner.log\", \"w\")\n",
    "validator_parts = callbacks.Validator(inputs_val, output_parts_val, batchsize, scores, \n",
    "                                      lambda pred: (np.vstack(pred[0]) > 0.5).astype(int).flatten(), \n",
    "                                      \"f1\", prefix=f\"trainlogs/ner-parts\", stream=logfile)\n",
    "validator_starts = callbacks.Validator(inputs_val, output_starts_val, batchsize, scores, \n",
    "                                       lambda pred: (np.vstack(pred[1]) > 0.5).astype(int).flatten(), \n",
    "                                       \"f1\", prefix=f\"trainlogs/ner-starts\", stream=logfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "model.fit(inputs, [output_parts, output_starts],\n",
    "          verbose=1, epochs=50, batch_size=batchsize,\n",
    "          initial_epoch=0, callbacks=[resetter, validator_parts, resetter, validator_starts])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
